
==> Audit <==
|--------------|---------------------|----------|-------|---------|---------------------|---------------------|
|   Command    |        Args         | Profile  | User  | Version |     Start Time      |      End Time       |
|--------------|---------------------|----------|-------|---------|---------------------|---------------------|
| start        | --driver=docker     | minikube | boris | v1.35.0 | 24 Feb 25 00:56 CET | 24 Feb 25 01:00 CET |
| update-check |                     | minikube | boris | v1.35.0 | 24 Feb 25 22:39 CET | 24 Feb 25 22:39 CET |
| update-check |                     | minikube | boris | v1.35.0 | 26 Feb 25 13:41 CET | 26 Feb 25 13:41 CET |
| start        |                     | minikube | boris | v1.35.0 | 02 Mar 25 14:41 CET | 02 Mar 25 14:41 CET |
| update-check |                     | minikube | boris | v1.35.0 | 02 Mar 25 14:43 CET | 02 Mar 25 14:43 CET |
| update-check |                     | minikube | boris | v1.35.0 | 02 Mar 25 14:45 CET | 02 Mar 25 14:45 CET |
| update-check |                     | minikube | boris | v1.35.0 | 02 Mar 25 14:57 CET | 02 Mar 25 14:57 CET |
| service      | nginx-service --url | minikube | boris | v1.35.0 | 02 Mar 25 14:59 CET |                     |
|--------------|---------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/02 14:41:33
Running on machine: Devops
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0302 14:41:33.961734   48475 out.go:345] Setting OutFile to fd 1 ...
I0302 14:41:33.962297   48475 out.go:397] isatty.IsTerminal(1) = true
I0302 14:41:33.962302   48475 out.go:358] Setting ErrFile to fd 2...
I0302 14:41:33.962307   48475 out.go:397] isatty.IsTerminal(2) = true
I0302 14:41:33.962415   48475 root.go:338] Updating PATH: /home/boris/.minikube/bin
W0302 14:41:33.963004   48475 root.go:314] Error reading config file at /home/boris/.minikube/config/config.json: open /home/boris/.minikube/config/config.json: no such file or directory
I0302 14:41:33.964181   48475 out.go:352] Setting JSON to false
I0302 14:41:33.977353   48475 start.go:129] hostinfo: {"hostname":"Devops","uptime":15384,"bootTime":1740907510,"procs":228,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"12.9","kernelVersion":"6.1.0-31-amd64","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"6ceddb3d-cb4f-43be-82ed-f939720f2429"}
I0302 14:41:33.977891   48475 start.go:139] virtualization: vbox guest
I0302 14:41:33.980194   48475 out.go:177] üòÑ  minikube v1.35.0 on Debian 12.9 (vbox/amd64)
I0302 14:41:33.983853   48475 notify.go:220] Checking for updates...
I0302 14:41:33.984017   48475 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0302 14:41:33.985382   48475 driver.go:394] Setting default libvirt URI to qemu:///system
I0302 14:41:34.008389   48475 docker.go:123] docker version: linux-27.5.1:Docker Engine - Community
I0302 14:41:34.008474   48475 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0302 14:41:34.058655   48475 info.go:266] docker info: {ID:14185d8f-63f4-47eb-82aa-aa1d967f7d44 Containers:10 ContainersRunning:0 ContainersPaused:0 ContainersStopped:10 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:42 SystemTime:2025-03-02 14:41:34.050804907 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-31-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:33655803904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Devops Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0302 14:41:34.058712   48475 docker.go:318] overlay module found
I0302 14:41:34.060604   48475 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0302 14:41:34.062553   48475 start.go:297] selected driver: docker
I0302 14:41:34.062557   48475 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/boris:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0302 14:41:34.062639   48475 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0302 14:41:34.062704   48475 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0302 14:41:34.120283   48475 info.go:266] docker info: {ID:14185d8f-63f4-47eb-82aa-aa1d967f7d44 Containers:10 ContainersRunning:0 ContainersPaused:0 ContainersStopped:10 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:42 SystemTime:2025-03-02 14:41:34.113095685 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-31-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:33655803904 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Devops Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0302 14:41:34.120880   48475 cni.go:84] Creating CNI manager for ""
I0302 14:41:34.121931   48475 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0302 14:41:34.121968   48475 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/boris:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0302 14:41:34.123654   48475 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0302 14:41:34.125042   48475 cache.go:121] Beginning downloading kic base image for docker with docker
I0302 14:41:34.126447   48475 out.go:177] üöú  Pulling base image v0.0.46 ...
I0302 14:41:34.127965   48475 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0302 14:41:34.128000   48475 preload.go:146] Found local preload: /home/boris/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0302 14:41:34.128018   48475 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0302 14:41:34.128025   48475 cache.go:56] Caching tarball of preloaded images
I0302 14:41:34.128136   48475 preload.go:172] Found /home/boris/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0302 14:41:34.128144   48475 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0302 14:41:34.128226   48475 profile.go:143] Saving config to /home/boris/.minikube/profiles/minikube/config.json ...
I0302 14:41:34.154793   48475 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0302 14:41:34.154803   48475 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0302 14:41:34.154818   48475 cache.go:227] Successfully downloaded all kic artifacts
I0302 14:41:34.154840   48475 start.go:360] acquireMachinesLock for minikube: {Name:mk2c668dbd587967f6df6a1a2ce2d05d091550ee Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0302 14:41:34.154942   48475 start.go:364] duration metric: took 71.169¬µs to acquireMachinesLock for "minikube"
I0302 14:41:34.154955   48475 start.go:96] Skipping create...Using existing machine configuration
I0302 14:41:34.154963   48475 fix.go:54] fixHost starting: 
I0302 14:41:34.155439   48475 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0302 14:41:34.172973   48475 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0302 14:41:34.173005   48475 fix.go:138] unexpected machine state, will restart: <nil>
I0302 14:41:34.174887   48475 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0302 14:41:34.176531   48475 cli_runner.go:164] Run: docker start minikube
I0302 14:41:34.467067   48475 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0302 14:41:34.481874   48475 kic.go:430] container "minikube" state is running.
I0302 14:41:34.482140   48475 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0302 14:41:34.498976   48475 profile.go:143] Saving config to /home/boris/.minikube/profiles/minikube/config.json ...
I0302 14:41:34.499183   48475 machine.go:93] provisionDockerMachine start ...
I0302 14:41:34.499221   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:34.514897   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:34.515612   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:34.515620   48475 main.go:141] libmachine: About to run SSH command:
hostname
I0302 14:41:34.516203   48475 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:44514->127.0.0.1:32768: read: connection reset by peer
I0302 14:41:37.664581   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0302 14:41:37.664597   48475 ubuntu.go:169] provisioning hostname "minikube"
I0302 14:41:37.665232   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:37.694967   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:37.695216   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:37.695233   48475 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0302 14:41:37.866343   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0302 14:41:37.866384   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:37.881159   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:37.881330   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:37.881351   48475 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0302 14:41:38.017739   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0302 14:41:38.017766   48475 ubuntu.go:175] set auth options {CertDir:/home/boris/.minikube CaCertPath:/home/boris/.minikube/certs/ca.pem CaPrivateKeyPath:/home/boris/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/boris/.minikube/machines/server.pem ServerKeyPath:/home/boris/.minikube/machines/server-key.pem ClientKeyPath:/home/boris/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/boris/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/boris/.minikube}
I0302 14:41:38.017781   48475 ubuntu.go:177] setting up certificates
I0302 14:41:38.017791   48475 provision.go:84] configureAuth start
I0302 14:41:38.017837   48475 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0302 14:41:38.039749   48475 provision.go:143] copyHostCerts
I0302 14:41:38.040440   48475 exec_runner.go:144] found /home/boris/.minikube/ca.pem, removing ...
I0302 14:41:38.041035   48475 exec_runner.go:203] rm: /home/boris/.minikube/ca.pem
I0302 14:41:38.041116   48475 exec_runner.go:151] cp: /home/boris/.minikube/certs/ca.pem --> /home/boris/.minikube/ca.pem (1074 bytes)
I0302 14:41:38.041834   48475 exec_runner.go:144] found /home/boris/.minikube/cert.pem, removing ...
I0302 14:41:38.041840   48475 exec_runner.go:203] rm: /home/boris/.minikube/cert.pem
I0302 14:41:38.041871   48475 exec_runner.go:151] cp: /home/boris/.minikube/certs/cert.pem --> /home/boris/.minikube/cert.pem (1119 bytes)
I0302 14:41:38.042493   48475 exec_runner.go:144] found /home/boris/.minikube/key.pem, removing ...
I0302 14:41:38.042499   48475 exec_runner.go:203] rm: /home/boris/.minikube/key.pem
I0302 14:41:38.042529   48475 exec_runner.go:151] cp: /home/boris/.minikube/certs/key.pem --> /home/boris/.minikube/key.pem (1679 bytes)
I0302 14:41:38.043148   48475 provision.go:117] generating server cert: /home/boris/.minikube/machines/server.pem ca-key=/home/boris/.minikube/certs/ca.pem private-key=/home/boris/.minikube/certs/ca-key.pem org=boris.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0302 14:41:38.161394   48475 provision.go:177] copyRemoteCerts
I0302 14:41:38.161431   48475 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0302 14:41:38.161458   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:38.174683   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:38.272476   48475 ssh_runner.go:362] scp /home/boris/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0302 14:41:38.301685   48475 ssh_runner.go:362] scp /home/boris/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0302 14:41:38.324912   48475 ssh_runner.go:362] scp /home/boris/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0302 14:41:38.344409   48475 provision.go:87] duration metric: took 326.609122ms to configureAuth
I0302 14:41:38.344421   48475 ubuntu.go:193] setting minikube options for container-runtime
I0302 14:41:38.344565   48475 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0302 14:41:38.344596   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:38.358676   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:38.358801   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:38.358809   48475 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0302 14:41:38.494166   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0302 14:41:38.494181   48475 ubuntu.go:71] root file system type: overlay
I0302 14:41:38.494351   48475 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0302 14:41:38.494428   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:38.524866   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:38.525042   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:38.525237   48475 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0302 14:41:38.691686   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0302 14:41:38.691772   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:38.721991   48475 main.go:141] libmachine: Using SSH client type: native
I0302 14:41:38.722166   48475 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0302 14:41:38.722199   48475 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0302 14:41:38.866339   48475 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0302 14:41:38.866350   48475 machine.go:96] duration metric: took 4.367161354s to provisionDockerMachine
I0302 14:41:38.866363   48475 start.go:293] postStartSetup for "minikube" (driver="docker")
I0302 14:41:38.866373   48475 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0302 14:41:38.866407   48475 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0302 14:41:38.866432   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:38.884741   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:38.985481   48475 ssh_runner.go:195] Run: cat /etc/os-release
I0302 14:41:38.988169   48475 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0302 14:41:38.988189   48475 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0302 14:41:38.988198   48475 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0302 14:41:38.988202   48475 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0302 14:41:38.988207   48475 filesync.go:126] Scanning /home/boris/.minikube/addons for local assets ...
I0302 14:41:38.988726   48475 filesync.go:126] Scanning /home/boris/.minikube/files for local assets ...
I0302 14:41:38.989136   48475 start.go:296] duration metric: took 122.767986ms for postStartSetup
I0302 14:41:38.989167   48475 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0302 14:41:38.989189   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:39.004393   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:39.098532   48475 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0302 14:41:39.107798   48475 fix.go:56] duration metric: took 4.952826415s for fixHost
I0302 14:41:39.107874   48475 start.go:83] releasing machines lock for "minikube", held for 4.952914471s
I0302 14:41:39.107971   48475 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0302 14:41:39.136809   48475 ssh_runner.go:195] Run: cat /version.json
I0302 14:41:39.136844   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:39.136852   48475 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0302 14:41:39.136893   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:39.153729   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:39.155296   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:39.243076   48475 ssh_runner.go:195] Run: systemctl --version
I0302 14:41:39.548282   48475 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0302 14:41:39.554027   48475 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0302 14:41:39.574223   48475 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0302 14:41:39.574260   48475 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0302 14:41:39.581728   48475 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0302 14:41:39.581739   48475 start.go:495] detecting cgroup driver to use...
I0302 14:41:39.581756   48475 detect.go:190] detected "systemd" cgroup driver on host os
I0302 14:41:39.581868   48475 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0302 14:41:39.595694   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0302 14:41:39.604242   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0302 14:41:39.612761   48475 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0302 14:41:39.612797   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0302 14:41:39.621730   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0302 14:41:39.629386   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0302 14:41:39.637546   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0302 14:41:39.645174   48475 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0302 14:41:39.657463   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0302 14:41:39.665933   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0302 14:41:39.674251   48475 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0302 14:41:39.682596   48475 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0302 14:41:39.691008   48475 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0302 14:41:39.691036   48475 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0302 14:41:39.704388   48475 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0302 14:41:39.712604   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:39.796593   48475 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0302 14:41:39.882939   48475 start.go:495] detecting cgroup driver to use...
I0302 14:41:39.882960   48475 detect.go:190] detected "systemd" cgroup driver on host os
I0302 14:41:39.883006   48475 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0302 14:41:39.904842   48475 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0302 14:41:39.904906   48475 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0302 14:41:39.918446   48475 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0302 14:41:39.934525   48475 ssh_runner.go:195] Run: which cri-dockerd
I0302 14:41:39.937597   48475 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0302 14:41:39.946537   48475 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0302 14:41:39.965082   48475 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0302 14:41:40.054494   48475 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0302 14:41:40.135878   48475 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0302 14:41:40.135942   48475 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0302 14:41:40.151808   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:40.245455   48475 ssh_runner.go:195] Run: sudo systemctl restart docker
I0302 14:41:42.311935   48475 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.066463793s)
I0302 14:41:42.311970   48475 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0302 14:41:42.322111   48475 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0302 14:41:42.334002   48475 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0302 14:41:42.348599   48475 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0302 14:41:42.449475   48475 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0302 14:41:42.539729   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:42.630763   48475 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0302 14:41:42.662038   48475 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0302 14:41:42.671235   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:42.769206   48475 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0302 14:41:42.971678   48475 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0302 14:41:42.971733   48475 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0302 14:41:42.974760   48475 start.go:563] Will wait 60s for crictl version
I0302 14:41:42.974785   48475 ssh_runner.go:195] Run: which crictl
I0302 14:41:42.977788   48475 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0302 14:41:43.076885   48475 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0302 14:41:43.076922   48475 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0302 14:41:43.149840   48475 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0302 14:41:43.171793   48475 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0302 14:41:43.171865   48475 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0302 14:41:43.187278   48475 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0302 14:41:43.189914   48475 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0302 14:41:43.201248   48475 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/boris:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0302 14:41:43.201330   48475 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0302 14:41:43.201403   48475 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0302 14:41:43.221962   48475 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0302 14:41:43.221971   48475 docker.go:619] Images already preloaded, skipping extraction
I0302 14:41:43.222022   48475 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0302 14:41:43.238230   48475 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0302 14:41:43.238238   48475 cache_images.go:84] Images are preloaded, skipping loading
I0302 14:41:43.238244   48475 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0302 14:41:43.238352   48475 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0302 14:41:43.238394   48475 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0302 14:41:43.396845   48475 cni.go:84] Creating CNI manager for ""
I0302 14:41:43.396855   48475 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0302 14:41:43.396922   48475 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0302 14:41:43.396938   48475 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0302 14:41:43.397034   48475 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0302 14:41:43.397072   48475 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0302 14:41:43.407402   48475 binaries.go:44] Found k8s binaries, skipping transfer
I0302 14:41:43.407436   48475 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0302 14:41:43.415237   48475 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0302 14:41:43.433534   48475 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0302 14:41:43.448101   48475 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0302 14:41:43.464391   48475 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0302 14:41:43.466740   48475 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0302 14:41:43.476423   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:43.574396   48475 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0302 14:41:43.606890   48475 certs.go:68] Setting up /home/boris/.minikube/profiles/minikube for IP: 192.168.49.2
I0302 14:41:43.606901   48475 certs.go:194] generating shared ca certs ...
I0302 14:41:43.606913   48475 certs.go:226] acquiring lock for ca certs: {Name:mkf7def16fbb1f6bd623780f825f9428b8aa5733 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0302 14:41:43.607196   48475 certs.go:235] skipping valid "minikubeCA" ca cert: /home/boris/.minikube/ca.key
I0302 14:41:43.607750   48475 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/boris/.minikube/proxy-client-ca.key
I0302 14:41:43.607757   48475 certs.go:256] generating profile certs ...
I0302 14:41:43.607828   48475 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/boris/.minikube/profiles/minikube/client.key
I0302 14:41:43.608252   48475 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/boris/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0302 14:41:43.608705   48475 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/boris/.minikube/profiles/minikube/proxy-client.key
I0302 14:41:43.608826   48475 certs.go:484] found cert: /home/boris/.minikube/certs/ca-key.pem (1675 bytes)
I0302 14:41:43.608849   48475 certs.go:484] found cert: /home/boris/.minikube/certs/ca.pem (1074 bytes)
I0302 14:41:43.608866   48475 certs.go:484] found cert: /home/boris/.minikube/certs/cert.pem (1119 bytes)
I0302 14:41:43.608883   48475 certs.go:484] found cert: /home/boris/.minikube/certs/key.pem (1679 bytes)
I0302 14:41:43.610425   48475 ssh_runner.go:362] scp /home/boris/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0302 14:41:43.638530   48475 ssh_runner.go:362] scp /home/boris/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0302 14:41:43.663875   48475 ssh_runner.go:362] scp /home/boris/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0302 14:41:43.689264   48475 ssh_runner.go:362] scp /home/boris/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0302 14:41:43.718485   48475 ssh_runner.go:362] scp /home/boris/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0302 14:41:43.746353   48475 ssh_runner.go:362] scp /home/boris/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0302 14:41:43.771705   48475 ssh_runner.go:362] scp /home/boris/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0302 14:41:43.796540   48475 ssh_runner.go:362] scp /home/boris/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0302 14:41:43.825083   48475 ssh_runner.go:362] scp /home/boris/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0302 14:41:43.850929   48475 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0302 14:41:43.868635   48475 ssh_runner.go:195] Run: openssl version
I0302 14:41:43.877168   48475 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0302 14:41:43.887586   48475 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0302 14:41:43.891379   48475 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 23 23:59 /usr/share/ca-certificates/minikubeCA.pem
I0302 14:41:43.891420   48475 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0302 14:41:43.898029   48475 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0302 14:41:43.911028   48475 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0302 14:41:43.914774   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0302 14:41:43.931565   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0302 14:41:43.941643   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0302 14:41:43.950267   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0302 14:41:43.958592   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0302 14:41:43.967582   48475 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0302 14:41:43.975825   48475 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/boris:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0302 14:41:43.975930   48475 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0302 14:41:43.996036   48475 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0302 14:41:44.008353   48475 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0302 14:41:44.008358   48475 kubeadm.go:593] restartPrimaryControlPlane start ...
I0302 14:41:44.008388   48475 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0302 14:41:44.021258   48475 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0302 14:41:44.022155   48475 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0302 14:41:44.033631   48475 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0302 14:41:44.044737   48475 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0302 14:41:44.044752   48475 kubeadm.go:597] duration metric: took 36.390653ms to restartPrimaryControlPlane
I0302 14:41:44.044758   48475 kubeadm.go:394] duration metric: took 68.945308ms to StartCluster
I0302 14:41:44.044769   48475 settings.go:142] acquiring lock: {Name:mkb14c07c326c77c279ab7edac787e8bd9b0888e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0302 14:41:44.044899   48475 settings.go:150] Updating kubeconfig:  /home/boris/.kube/config
I0302 14:41:44.045390   48475 lock.go:35] WriteFile acquiring /home/boris/.kube/config: {Name:mkaf67f47c3eb46aca95d8ce1d7876a3f720e876 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0302 14:41:44.045671   48475 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0302 14:41:44.045990   48475 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0302 14:41:44.046006   48475 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0302 14:41:44.046054   48475 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0302 14:41:44.046062   48475 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0302 14:41:44.046066   48475 addons.go:247] addon storage-provisioner should already be in state true
I0302 14:41:44.046079   48475 host.go:66] Checking if "minikube" exists ...
I0302 14:41:44.046298   48475 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0302 14:41:44.046348   48475 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0302 14:41:44.046669   48475 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0302 14:41:44.046698   48475 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0302 14:41:44.048550   48475 out.go:177] üîé  Verifying Kubernetes components...
I0302 14:41:44.050119   48475 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0302 14:41:44.067249   48475 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0302 14:41:44.067257   48475 addons.go:247] addon default-storageclass should already be in state true
I0302 14:41:44.067274   48475 host.go:66] Checking if "minikube" exists ...
I0302 14:41:44.067830   48475 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0302 14:41:44.070241   48475 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0302 14:41:44.074154   48475 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0302 14:41:44.074168   48475 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0302 14:41:44.074234   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:44.103707   48475 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0302 14:41:44.103716   48475 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0302 14:41:44.103753   48475 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0302 14:41:44.104533   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:44.122640   48475 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/boris/.minikube/machines/minikube/id_rsa Username:docker}
I0302 14:41:44.161127   48475 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0302 14:41:44.197654   48475 api_server.go:52] waiting for apiserver process to appear ...
I0302 14:41:44.197700   48475 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0302 14:41:44.214449   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0302 14:41:44.232625   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0302 14:41:44.383933   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.383953   48475 retry.go:31] will retry after 336.540902ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0302 14:41:44.385489   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.385495   48475 retry.go:31] will retry after 240.736423ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.626719   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0302 14:41:44.691979   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.691994   48475 retry.go:31] will retry after 213.180615ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.698330   48475 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0302 14:41:44.721813   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0302 14:41:44.814458   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.814470   48475 retry.go:31] will retry after 417.166896ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.906056   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0302 14:41:44.977168   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:44.977181   48475 retry.go:31] will retry after 343.634479ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:45.198639   48475 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0302 14:41:45.211523   48475 api_server.go:72] duration metric: took 1.165833614s to wait for apiserver process to appear ...
I0302 14:41:45.211533   48475 api_server.go:88] waiting for apiserver healthz status ...
I0302 14:41:45.211549   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:45.211858   48475 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0302 14:41:45.232162   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0302 14:41:45.299718   48475 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:45.299731   48475 retry.go:31] will retry after 841.058356ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0302 14:41:45.321192   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0302 14:41:45.711596   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:46.140964   48475 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0302 14:41:47.707302   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0302 14:41:47.707313   48475 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0302 14:41:47.707321   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:47.745288   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0302 14:41:47.745300   48475 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0302 14:41:47.745310   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:47.752477   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0302 14:41:47.752488   48475 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0302 14:41:47.823113   48475 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.501903458s)
I0302 14:41:48.211785   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:48.215071   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0302 14:41:48.215081   48475 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0302 14:41:48.571753   48475 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.430772555s)
I0302 14:41:48.576086   48475 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I0302 14:41:48.577533   48475 addons.go:514] duration metric: took 4.531527303s for enable addons: enabled=[default-storageclass storage-provisioner]
I0302 14:41:48.713048   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:48.719788   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0302 14:41:48.719797   48475 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0302 14:41:49.214465   48475 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0302 14:41:49.217916   48475 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0302 14:41:49.218566   48475 api_server.go:141] control plane version: v1.32.0
I0302 14:41:49.218575   48475 api_server.go:131] duration metric: took 4.007037727s to wait for apiserver health ...
I0302 14:41:49.218586   48475 system_pods.go:43] waiting for kube-system pods to appear ...
I0302 14:41:49.229827   48475 system_pods.go:59] 7 kube-system pods found
I0302 14:41:49.229850   48475 system_pods.go:61] "coredns-668d6bf9bc-gj64w" [920c18d5-f10f-4b5b-8cd6-250a23b34cf1] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0302 14:41:49.229856   48475 system_pods.go:61] "etcd-minikube" [c71a0780-5b66-4684-9088-95ee9d82b54c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0302 14:41:49.229861   48475 system_pods.go:61] "kube-apiserver-minikube" [50d0c688-4ff5-4a2a-914d-4095ff9f1458] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0302 14:41:49.229866   48475 system_pods.go:61] "kube-controller-manager-minikube" [600c8c8d-28a5-4fe7-bdfb-695d95efe677] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0302 14:41:49.229871   48475 system_pods.go:61] "kube-proxy-fp5c4" [04d0f34a-9476-4252-a5fe-cd9a10b2774f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0302 14:41:49.229875   48475 system_pods.go:61] "kube-scheduler-minikube" [8a37bf5c-2017-444a-8196-66fa8f4a1e03] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0302 14:41:49.229879   48475 system_pods.go:61] "storage-provisioner" [d53bce37-687b-49c0-9931-bd946ce31f89] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0302 14:41:49.229885   48475 system_pods.go:74] duration metric: took 11.294143ms to wait for pod list to return data ...
I0302 14:41:49.229893   48475 kubeadm.go:582] duration metric: took 5.184205317s to wait for: map[apiserver:true system_pods:true]
I0302 14:41:49.229902   48475 node_conditions.go:102] verifying NodePressure condition ...
I0302 14:41:49.233351   48475 node_conditions.go:122] node storage ephemeral capacity is 109520332Ki
I0302 14:41:49.233362   48475 node_conditions.go:123] node cpu capacity is 4
I0302 14:41:49.233369   48475 node_conditions.go:105] duration metric: took 3.464438ms to run NodePressure ...
I0302 14:41:49.233379   48475 start.go:241] waiting for startup goroutines ...
I0302 14:41:49.233387   48475 start.go:246] waiting for cluster config update ...
I0302 14:41:49.233396   48475 start.go:255] writing updated cluster config ...
I0302 14:41:49.233607   48475 ssh_runner.go:195] Run: rm -f paused
I0302 14:41:49.496531   48475 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0302 14:41:49.498355   48475 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 02 13:41:41 minikube systemd[1]: Stopped Docker Application Container Engine.
Mar 02 13:41:41 minikube systemd[1]: Starting Docker Application Container Engine...
Mar 02 13:41:41 minikube dockerd[953]: time="2025-03-02T13:41:41.052271627Z" level=info msg="Starting up"
Mar 02 13:41:41 minikube dockerd[953]: time="2025-03-02T13:41:41.053125900Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Mar 02 13:41:41 minikube dockerd[953]: time="2025-03-02T13:41:41.068658346Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Mar 02 13:41:41 minikube dockerd[953]: time="2025-03-02T13:41:41.077454641Z" level=info msg="Loading containers: start."
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.079590396Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.276826684Z" level=info msg="Loading containers: done."
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.287550191Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.287607602Z" level=info msg="Daemon has completed initialization"
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.310192368Z" level=info msg="API listen on /var/run/docker.sock"
Mar 02 13:41:42 minikube dockerd[953]: time="2025-03-02T13:41:42.310301954Z" level=info msg="API listen on [::]:2376"
Mar 02 13:41:42 minikube systemd[1]: Started Docker Application Container Engine.
Mar 02 13:41:42 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Start docker client with request timeout 0s"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Hairpin mode is set to hairpin-veth"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Loaded network plugin cni"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Docker cri networking managed by network plugin cni"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Setting cgroupDriver systemd"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 02 13:41:42 minikube cri-dockerd[1227]: time="2025-03-02T13:41:42Z" level=info msg="Start cri-dockerd grpc backend"
Mar 02 13:41:42 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 02 13:41:44 minikube cri-dockerd[1227]: time="2025-03-02T13:41:44Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-gj64w_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8a589c4802ef6146414e32132fffc056faf791e232f0bfa16b1a1e3c0da198d0\""
Mar 02 13:41:44 minikube cri-dockerd[1227]: time="2025-03-02T13:41:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c3709fd331e8dbe23fa749c458f40f9fc9a4e5bfea4ec045bf7539d6ff4d5dab/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:44 minikube cri-dockerd[1227]: time="2025-03-02T13:41:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22d4716f40ccaa87f88900fbbc9bb69c25fd49f0bf08a6130e8ec8b7cef8bafb/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:44 minikube cri-dockerd[1227]: time="2025-03-02T13:41:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/832b4c6150a1c2203129a7769511ca0733f76bc7ce512c4d915401fefbe70c36/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:44 minikube cri-dockerd[1227]: time="2025-03-02T13:41:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a572859e950bdf2dbe74343c210fa3e53626690c2cef7f336859795e9c26dc97/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:47 minikube cri-dockerd[1227]: time="2025-03-02T13:41:47Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 02 13:41:49 minikube cri-dockerd[1227]: time="2025-03-02T13:41:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b70891d99772377bda9bbc930df786e984a4b21b812fdf329421ff6b880380ff/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:49 minikube cri-dockerd[1227]: time="2025-03-02T13:41:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/61b90db192d670e10500e930579da817d390e450f8038510e7b2093a454aa89b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:41:49 minikube cri-dockerd[1227]: time="2025-03-02T13:41:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b03466fd0ed5e3cbac209f948730a350a97e23d6f02885b556b8daf4253f64e/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 02 13:42:10 minikube dockerd[953]: time="2025-03-02T13:42:10.771138286Z" level=info msg="ignoring event" container=993315699ba2d80125a385365cadd5e01d3af8547eb09e05de7be4969ea2db55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:46:29 minikube cri-dockerd[1227]: time="2025-03-02T13:46:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/24956e688af1477cd4e1c7c2a482796db082403300e60a71dc81247cfdb4d5f4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:46:36 minikube cri-dockerd[1227]: time="2025-03-02T13:46:36Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Mar 02 13:48:34 minikube dockerd[953]: time="2025-03-02T13:48:34.273593099Z" level=info msg="ignoring event" container=6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:48:34 minikube dockerd[953]: time="2025-03-02T13:48:34.514469379Z" level=info msg="ignoring event" container=24956e688af1477cd4e1c7c2a482796db082403300e60a71dc81247cfdb4d5f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:50:24 minikube cri-dockerd[1227]: time="2025-03-02T13:50:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c9d12dd36f3ff481bc9d9207e6b93dcba6e1846f977331e942b4f00f6b7c7ca3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:50:24 minikube cri-dockerd[1227]: time="2025-03-02T13:50:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/77a370b298c14fdf7ec44e7bfd2297f4c033bfcc4f43eae1a5db25ebb4aee83c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:50:24 minikube cri-dockerd[1227]: time="2025-03-02T13:50:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e3c4e5bc58ce6743502abb4b08d65b5ae0b9256ab98e3e26f39063abb1721fa2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:50:25 minikube cri-dockerd[1227]: time="2025-03-02T13:50:25Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Mar 02 13:50:26 minikube cri-dockerd[1227]: time="2025-03-02T13:50:26Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Mar 02 13:50:28 minikube cri-dockerd[1227]: time="2025-03-02T13:50:28Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Mar 02 13:50:59 minikube cri-dockerd[1227]: time="2025-03-02T13:50:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f904a6c32eb34066f48c519978b3b263d8d1b9ee615e0ee18d857182bd32d746/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:50:59 minikube cri-dockerd[1227]: time="2025-03-02T13:50:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b52d4a777d98a9b4823f7eaea45ab4f773af86caf5da3bbf17f06c02d804c543/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 02 13:51:00 minikube cri-dockerd[1227]: time="2025-03-02T13:51:00Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Mar 02 13:51:02 minikube cri-dockerd[1227]: time="2025-03-02T13:51:02Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.355569368Z" level=info msg="ignoring event" container=f3462a1619f767dd4e99d5ef37b4a8c96950b06903d8de2862db9623375e8138 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.390103525Z" level=info msg="ignoring event" container=59648c5c1846ce3fdbb1c8b9db8ba165ae29d0d48470d2c592b666cf1aae3809 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.438544040Z" level=info msg="ignoring event" container=f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.439150944Z" level=info msg="ignoring event" container=ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.445120325Z" level=info msg="ignoring event" container=a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.598728420Z" level=info msg="ignoring event" container=c9d12dd36f3ff481bc9d9207e6b93dcba6e1846f977331e942b4f00f6b7c7ca3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube cri-dockerd[1227]: time="2025-03-02T13:51:19Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-deployment-96b9d695-jzjk8_default\": unexpected command output nsenter: cannot open /proc/4824/ns/net: No such file or directory\n with error: exit status 1"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.787342181Z" level=info msg="ignoring event" container=77a370b298c14fdf7ec44e7bfd2297f4c033bfcc4f43eae1a5db25ebb4aee83c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.855870616Z" level=info msg="ignoring event" container=b52d4a777d98a9b4823f7eaea45ab4f773af86caf5da3bbf17f06c02d804c543 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.921494472Z" level=info msg="ignoring event" container=f904a6c32eb34066f48c519978b3b263d8d1b9ee615e0ee18d857182bd32d746 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 02 13:51:19 minikube dockerd[953]: time="2025-03-02T13:51:19.946398316Z" level=info msg="ignoring event" container=e3c4e5bc58ce6743502abb4b08d65b5ae0b9256ab98e3e26f39063abb1721fa2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ea3754f3dd735       6e38f40d628db       17 minutes ago      Running             storage-provisioner       3                   b70891d997723       storage-provisioner
36462b815221c       c69fa2e9cbf5f       18 minutes ago      Running             coredns                   1                   9b03466fd0ed5       coredns-668d6bf9bc-gj64w
23691dd96a71b       040f9f8aac8cd       18 minutes ago      Running             kube-proxy                1                   61b90db192d67       kube-proxy-fp5c4
993315699ba2d       6e38f40d628db       18 minutes ago      Exited              storage-provisioner       2                   b70891d997723       storage-provisioner
e1c0e7703dc8e       c2e17b8d0f4a3       18 minutes ago      Running             kube-apiserver            1                   a572859e950bd       kube-apiserver-minikube
ea1d9425254d6       a9e7e6b294baf       18 minutes ago      Running             etcd                      1                   832b4c6150a1c       etcd-minikube
61bc506a9aa4a       8cab3d2a8bd0f       18 minutes ago      Running             kube-controller-manager   1                   22d4716f40cca       kube-controller-manager-minikube
97c02cdae1517       a389e107f4ff1       18 minutes ago      Running             kube-scheduler            1                   c3709fd331e8d       kube-scheduler-minikube
5a9f5b967ab94       c69fa2e9cbf5f       6 days ago          Exited              coredns                   0                   8a589c4802ef6       coredns-668d6bf9bc-gj64w
655fcd06df9b7       040f9f8aac8cd       6 days ago          Exited              kube-proxy                0                   f053bd2ebc733       kube-proxy-fp5c4
d3d42628da69e       8cab3d2a8bd0f       6 days ago          Exited              kube-controller-manager   0                   e0a6be939e856       kube-controller-manager-minikube
86dac78ae57af       a9e7e6b294baf       6 days ago          Exited              etcd                      0                   48130d2c222e0       etcd-minikube
09296395f8efc       a389e107f4ff1       6 days ago          Exited              kube-scheduler            0                   2d916c3afb934       kube-scheduler-minikube
98fedd2744154       c2e17b8d0f4a3       6 days ago          Exited              kube-apiserver            0                   83724f9b089c4       kube-apiserver-minikube


==> coredns [36462b815221] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:50381 - 19873 "HINFO IN 5035488728456232967.6329499895457469599. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.021352137s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[816185941]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (02-Mar-2025 13:41:49.972) (total time: 21006ms):
Trace[816185941]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21006ms (13:42:10.978)
Trace[816185941]: [21.006983626s] [21.006983626s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1674085383]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (02-Mar-2025 13:41:49.972) (total time: 21006ms):
Trace[1674085383]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21006ms (13:42:10.978)
Trace[1674085383]: [21.006710069s] [21.006710069s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[801427132]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (02-Mar-2025 13:41:49.972) (total time: 21006ms):
Trace[801427132]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21006ms (13:42:10.978)
Trace[801427132]: [21.006620235s] [21.006620235s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [5a9f5b967ab9] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:51949 - 22451 "HINFO IN 6265904032658136542.1749209429528204743. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.046842612s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1786960418]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (24-Feb-2025 00:00:06.965) (total time: 21017ms):
Trace[1786960418]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21014ms (00:00:27.979)
Trace[1786960418]: [21.017942026s] [21.017942026s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[396013175]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (24-Feb-2025 00:00:06.965) (total time: 21018ms):
Trace[396013175]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21014ms (00:00:27.979)
Trace[396013175]: [21.018640963s] [21.018640963s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[939034207]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (24-Feb-2025 00:00:06.967) (total time: 21017ms):
Trace[939034207]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21014ms (00:00:27.981)
Trace[939034207]: [21.017056173s] [21.017056173s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_24T01_00_01_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 23 Feb 2025 23:59:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 02 Mar 2025 14:00:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 02 Mar 2025 13:58:59 +0000   Sun, 23 Feb 2025 23:59:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 02 Mar 2025 13:58:59 +0000   Sun, 23 Feb 2025 23:59:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 02 Mar 2025 13:58:59 +0000   Sun, 23 Feb 2025 23:59:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 02 Mar 2025 13:58:59 +0000   Sun, 23 Feb 2025 23:59:54 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  109520332Ki
  hugepages-2Mi:      0
  memory:             32866996Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  109520332Ki
  hugepages-2Mi:      0
  memory:             32866996Ki
  pods:               110
System Info:
  Machine ID:                 5c7fe0e808354e4c8731478dac471822
  System UUID:                a97f9951-57ae-4dc2-9ce3-57bce6b9ef03
  Boot ID:                    2e2f5966-b867-46d1-bf6d-aaf01ea979e0
  Kernel Version:             6.1.0-31-amd64
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-668d6bf9bc-gj64w            100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d14h
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         6d14h
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         6d14h
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         6d14h
  kube-system                 kube-proxy-fp5c4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d14h
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         6d14h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d14h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (0%)  170Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           6d14h                  kube-proxy       
  Normal   Starting                           18m                    kube-proxy       
  Normal   NodeHasSufficientMemory            6d14h (x8 over 6d14h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              6d14h (x8 over 6d14h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               6d14h (x7 over 6d14h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            6d14h                  kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  6d14h                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           6d14h                  kubelet          Starting kubelet.
  Normal   NodeHasNoDiskPressure              6d14h                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory            6d14h                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID               6d14h                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            6d14h                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     6d14h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  18m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           18m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory            18m (x8 over 18m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              18m (x8 over 18m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               18m (x7 over 18m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            18m                    kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           18m                    kubelet          Node minikube has been rebooted, boot id: 2e2f5966-b867-46d1-bf6d-aaf01ea979e0
  Normal   RegisteredNode                     18m                    node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
              17:46:17.479439 main     Package type: LINUX_64BITS_GENERIC
[  +0.002951] 17:46:17.482376 main     7.1.4 r165100 started. Verbose level = 0
[  +0.004273] 17:46:17.486622 main     vbglR3GuestCtrlDetectPeekGetCancelSupport: Supported (#1)
[  +0.999017] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000044] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.396588] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000021] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.999715] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000020] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.554557] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000028] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 09:31] systemd-journald[258]: File /var/log/journal/6ceddb3dcb4f43be82edf939720f2429/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.820290] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000021] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +1.152705] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000022] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.185479] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000020] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.463450] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000023] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 09:35] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000021] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 10:13] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000024] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 11:47] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000024] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 12:04] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000024] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 12:26] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000023] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 12:34] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000023] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[ +17.897493] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000021] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 12:50] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 1008431939 wd_nsec: 1008431380
[  +3.008737] 21:11:50.535622 control  Error: GstCtrl: Getting host message failed with VERR_INTERRUPTED
[  +5.959333] 21:11:56.494804 control  Session 0 is about to close ...
[  +0.000882] 21:11:56.495175 control  Stopping all guest processes ...
[  +0.000046] 21:11:56.496051 control  Closing all guest files ...
[  +0.000114] 21:11:56.496173 control  vbglR3GuestCtrlDetectPeekGetCancelSupport: Supported (#1)
[  +0.324394] e1000 0000:00:03.0 enp0s3: Reset adapter
[  +3.715387] 23:56:52.876143 timesync vgsvcTimeSyncWorker: Radical host time change: 9 901 793 000 000ns (HostNow=1 740 787 012 876 000 000 ns HostLast=1 740 777 111 083 000 000 ns)
[Mar 2 12:51] 23:57:02.876967 timesync vgsvcTimeSyncWorker: Radical guest time change: 9 902 341 096 000ns (GuestNow=1 740 787 022 876 934 000 ns GuestLast=1 740 777 120 535 838 000 ns fSetTimeLastLoop=true)
[Mar 2 13:17] process '/bin/sh' started with executable stack
[Mar 2 13:38] 00:44:04.557637 control  Session 0 is about to close ...
[  +0.000120] 00:44:04.558001 control  Stopping all guest processes ...
[  +0.000041] 00:44:04.558115 control  Closing all guest files ...
[  +0.000110] 00:44:04.558212 control  vbglR3GuestCtrlDetectPeekGetCancelSupport: Supported (#1)
[  +9.096602] 13:38:18.492690 timesync vgsvcTimeSyncWorker: Radical host time change: 132 854 614 000 000ns (HostNow=1 740 922 698 492 000 000 ns HostLast=1 740 789 843 878 000 000 ns)
[  +0.167297] kauditd_printk_skb: 2 callbacks suppressed
[  +9.661228] 13:38:28.493044 timesync vgsvcTimeSyncWorker: Radical guest time change: 132 854 838 785 000ns (GuestNow=1 740 922 708 493 037 000 ns GuestLast=1 740 789 853 654 252 000 ns fSetTimeLastLoop=true)
[Mar 2 13:41] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000023] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[ +26.572868] tmpfs: Unknown parameter 'noswap'
[Mar 2 13:45] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000025] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[ +13.356050] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000023] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[Mar 2 13:55] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.
[  +0.000024] [drm:vmw_msg_ioctl [vmwgfx]] *ERROR* Failed to open channel.


==> etcd [86dac78ae57a] <==
{"level":"info","ts":"2025-02-23T23:59:50.235598Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-23T23:59:50.239267Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-23T23:59:50.239319Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-23T23:59:50.249467Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-23T23:59:50.249558Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-23T23:59:50.958199Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-02-23T23:59:50.958239Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-02-23T23:59:50.959415Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-02-23T23:59:50.959485Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-02-23T23:59:50.959493Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-23T23:59:50.959500Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-02-23T23:59:50.959506Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-23T23:59:50.979912Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-23T23:59:51.005250Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-23T23:59:51.008154Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-23T23:59:51.008358Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-23T23:59:51.019308Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-23T23:59:51.019454Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-23T23:59:51.019889Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-23T23:59:51.020553Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-23T23:59:51.011065Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-23T23:59:51.021242Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-23T23:59:51.021703Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-23T23:59:51.033217Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-23T23:59:51.033245Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"warn","ts":"2025-02-23T23:59:53.966380Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.775177ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-02-23T23:59:53.966450Z","caller":"traceutil/trace.go:171","msg":"trace[732849129] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:16; }","duration":"114.859892ms","start":"2025-02-23T23:59:53.851583Z","end":"2025-02-23T23:59:53.966443Z","steps":["trace[732849129] 'agreement among raft nodes before linearized reading'  (duration: 114.779364ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T23:59:53.966527Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.999041ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-02-23T23:59:53.966539Z","caller":"traceutil/trace.go:171","msg":"trace[205677295] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:0; response_revision:16; }","duration":"115.02655ms","start":"2025-02-23T23:59:53.851509Z","end":"2025-02-23T23:59:53.966535Z","steps":["trace[205677295] 'agreement among raft nodes before linearized reading'  (duration: 115.005172ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T23:59:53.966645Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.812967ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:2814"}
{"level":"info","ts":"2025-02-23T23:59:53.966658Z","caller":"traceutil/trace.go:171","msg":"trace[1291595793] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:16; }","duration":"119.836843ms","start":"2025-02-23T23:59:53.846818Z","end":"2025-02-23T23:59:53.966654Z","steps":["trace[1291595793] 'agreement among raft nodes before linearized reading'  (duration: 119.811749ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T23:59:54.430374Z","caller":"traceutil/trace.go:171","msg":"trace[979862254] transaction","detail":"{read_only:false; response_revision:39; number_of_response:1; }","duration":"105.084004ms","start":"2025-02-23T23:59:54.325279Z","end":"2025-02-23T23:59:54.430363Z","steps":["trace[979862254] 'process raft request'  (duration: 48.655631ms)","trace[979862254] 'compare'  (duration: 55.582022ms)"],"step_count":2}
{"level":"info","ts":"2025-02-24T00:09:51.052513Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":639}
{"level":"info","ts":"2025-02-24T00:09:51.065002Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":639,"took":"9.714403ms","hash":652755313,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1396736,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-02-24T00:09:51.065039Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":652755313,"revision":639,"compact-revision":-1}
{"level":"info","ts":"2025-02-24T00:14:51.072637Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":876}
{"level":"info","ts":"2025-02-24T00:14:51.080056Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":876,"took":"7.085117ms","hash":186490077,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":888832,"current-db-size-in-use":"889 kB"}
{"level":"info","ts":"2025-02-24T00:14:51.080545Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":186490077,"revision":876,"compact-revision":639}
{"level":"info","ts":"2025-02-24T00:19:51.095291Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1116}
{"level":"info","ts":"2025-02-24T00:19:51.102192Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1116,"took":"6.598298ms","hash":3574085232,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2025-02-24T00:19:51.102269Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3574085232,"revision":1116,"compact-revision":876}
{"level":"info","ts":"2025-02-24T00:24:51.118906Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1356}
{"level":"info","ts":"2025-02-24T00:24:51.126524Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1356,"took":"6.05916ms","hash":1945896881,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2025-02-24T00:24:51.126618Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1945896881,"revision":1356,"compact-revision":1116}
{"level":"info","ts":"2025-02-24T00:29:51.134848Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1594}
{"level":"info","ts":"2025-02-24T00:29:51.139837Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1594,"took":"4.736394ms","hash":3480564422,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2025-02-24T00:29:51.139920Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3480564422,"revision":1594,"compact-revision":1356}
{"level":"info","ts":"2025-02-24T00:34:51.156616Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1835}
{"level":"info","ts":"2025-02-24T00:34:51.166224Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1835,"took":"6.535613ms","hash":1775483728,"current-db-size-bytes":1396736,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2025-02-24T00:34:51.166308Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1775483728,"revision":1835,"compact-revision":1594}
{"level":"info","ts":"2025-02-24T00:35:40.789630Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-02-24T00:35:40.789853Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-02-24T00:35:40.789908Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-24T00:35:40.789968Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-24T00:35:40.804888Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-02-24T00:35:40.805041Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-02-24T00:35:40.805083Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-02-24T00:35:40.810754Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-24T00:35:40.810977Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-24T00:35:40.811040Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [ea1d9425254d] <==
{"level":"warn","ts":"2025-03-02T13:41:45.151994Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-02T13:41:45.152471Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-03-02T13:41:45.152525Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-03-02T13:41:45.152546Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-02T13:41:45.152553Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-02T13:41:45.152572Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-02T13:41:45.156253Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-03-02T13:41:45.157185Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-02T13:41:45.169236Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"11.340203ms"}
{"level":"info","ts":"2025-03-02T13:41:45.179783Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-03-02T13:41:45.188337Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2561}
{"level":"info","ts":"2025-03-02T13:41:45.188906Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-03-02T13:41:45.188931Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-03-02T13:41:45.188945Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 2561, applied: 0, lastindex: 2561, lastterm: 2]"}
{"level":"warn","ts":"2025-03-02T13:41:45.191777Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-02T13:41:45.193786Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1835}
{"level":"info","ts":"2025-03-02T13:41:45.197090Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":2117}
{"level":"info","ts":"2025-03-02T13:41:45.199636Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-02T13:41:45.202862Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-03-02T13:41:45.203106Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-03-02T13:41:45.203134Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-02T13:41:45.203422Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-03-02T13:41:45.203843Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-02T13:41:45.204058Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-02T13:41:45.204111Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-02T13:41:45.204118Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-02T13:41:45.203867Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-03-02T13:41:45.204274Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-02T13:41:45.204417Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-02T13:41:45.204515Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-02T13:41:45.210887Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-02T13:41:45.211178Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-02T13:41:45.211228Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-02T13:41:45.211281Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-02T13:41:45.211288Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-02T13:41:46.791739Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-03-02T13:41:46.791835Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-03-02T13:41:46.791871Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-02T13:41:46.791894Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-03-02T13:41:46.791908Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-02T13:41:46.791933Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-03-02T13:41:46.791948Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-02T13:41:46.794808Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-02T13:41:46.794922Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-02T13:41:46.795030Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-02T13:41:46.795527Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-02T13:41:46.795668Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-02T13:41:46.797910Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-02T13:41:46.798541Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-02T13:41:46.799956Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-02T13:41:46.801090Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-03-02T13:51:46.824692Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2442}
{"level":"info","ts":"2025-03-02T13:51:46.830055Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2442,"took":"5.150192ms","hash":2199650661,"current-db-size-bytes":2224128,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":2224128,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-03-02T13:51:46.830133Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2199650661,"revision":2442,"compact-revision":1835}
{"level":"info","ts":"2025-03-02T13:56:46.832557Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2791}
{"level":"info","ts":"2025-03-02T13:56:46.837025Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2791,"took":"4.20145ms","hash":2675545591,"current-db-size-bytes":2224128,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1769472,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-03-02T13:56:46.837069Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2675545591,"revision":2791,"compact-revision":2442}


==> kernel <==
 14:00:24 up  4:35,  0 users,  load average: 1.35, 1.04, 0.75
Linux minikube 6.1.0-31-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.128-1 (2025-02-07) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [98fedd274415] <==
W0224 00:35:40.802490       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:40.802515       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:40.802539       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:40.802686       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:40.802715       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:40.803372       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.797037       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.797018       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.797043       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.798690       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.798792       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.798939       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.798975       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799022       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799021       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799012       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799063       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799019       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.799053       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.800485       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802314       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802364       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802402       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802441       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802467       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802331       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802468       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802485       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802503       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802511       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802547       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802693       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802687       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802724       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802674       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802754       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802788       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802782       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802818       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802820       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802845       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802872       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802858       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802902       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802892       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802926       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802926       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802953       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802897       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802979       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802980       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.802994       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803011       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803022       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803038       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803009       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803051       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803081       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.803111       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0224 00:35:41.804399       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [e1c0e7703dc8] <==
I0302 13:41:47.665020       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0302 13:41:47.665357       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0302 13:41:47.668649       1 aggregator.go:169] waiting for initial CRD sync...
I0302 13:41:47.668803       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0302 13:41:47.668814       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0302 13:41:47.679304       1 controller.go:78] Starting OpenAPI AggregationController
I0302 13:41:47.679343       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0302 13:41:47.679676       1 controller.go:119] Starting legacy_token_tracking_controller
I0302 13:41:47.679688       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0302 13:41:47.679991       1 local_available_controller.go:156] Starting LocalAvailability controller
I0302 13:41:47.680001       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0302 13:41:47.680242       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0302 13:41:47.680331       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0302 13:41:47.680405       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0302 13:41:47.680410       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0302 13:41:47.680450       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0302 13:41:47.680690       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0302 13:41:47.680718       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0302 13:41:47.680723       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0302 13:41:47.682971       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0302 13:41:47.682982       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0302 13:41:47.682995       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0302 13:41:47.683015       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0302 13:41:47.683465       1 controller.go:142] Starting OpenAPI controller
I0302 13:41:47.683485       1 controller.go:90] Starting OpenAPI V3 controller
I0302 13:41:47.684004       1 naming_controller.go:294] Starting NamingConditionController
I0302 13:41:47.684017       1 establishing_controller.go:81] Starting EstablishingController
I0302 13:41:47.684027       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0302 13:41:47.684035       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0302 13:41:47.684043       1 crd_finalizer.go:269] Starting CRDFinalizer
I0302 13:41:47.679308       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0302 13:41:47.727717       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0302 13:41:47.727927       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0302 13:41:47.769058       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0302 13:41:47.780488       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0302 13:41:47.780515       1 shared_informer.go:320] Caches are synced for configmaps
I0302 13:41:47.781158       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0302 13:41:47.781166       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0302 13:41:47.781217       1 cache.go:39] Caches are synced for LocalAvailability controller
I0302 13:41:47.782583       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0302 13:41:47.782716       1 aggregator.go:171] initial CRD sync complete...
I0302 13:41:47.782727       1 autoregister_controller.go:144] Starting autoregister controller
I0302 13:41:47.782730       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0302 13:41:47.782734       1 cache.go:39] Caches are synced for autoregister controller
I0302 13:41:47.783960       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0302 13:41:47.784330       1 shared_informer.go:320] Caches are synced for node_authorizer
I0302 13:41:47.789658       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0302 13:41:47.803075       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0302 13:41:47.803097       1 policy_source.go:240] refreshing policies
I0302 13:41:47.827582       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0302 13:41:48.416643       1 controller.go:615] quota admission added evaluator for: endpoints
I0302 13:41:48.684110       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0302 13:41:48.877671       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0302 13:41:48.883322       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0302 13:41:49.026394       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0302 13:41:51.523999       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0302 13:41:51.623159       1 controller.go:615] quota admission added evaluator for: deployments.apps
E0302 13:41:58.161412       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 0262d0e8-9492-4e68-8233-f400fa3f92c3, UID in object meta: "
E0302 13:47:58.801066       1 conn.go:339] Error on socket receive: read tcp 192.168.49.2:8443->192.168.49.1:42474: use of closed network connection
I0302 13:58:15.892353       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.108.250.191"}


==> kube-controller-manager [61bc506a9aa4] <==
I0302 13:41:51.078971       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0302 13:41:51.080188       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0302 13:41:51.082054       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0302 13:41:51.084604       1 shared_informer.go:320] Caches are synced for persistent volume
I0302 13:41:51.084819       1 shared_informer.go:320] Caches are synced for GC
I0302 13:41:51.086313       1 shared_informer.go:320] Caches are synced for daemon sets
I0302 13:41:51.092734       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0302 13:41:51.094691       1 shared_informer.go:320] Caches are synced for ReplicationController
I0302 13:41:51.096852       1 shared_informer.go:320] Caches are synced for cronjob
I0302 13:41:51.098650       1 shared_informer.go:320] Caches are synced for TTL
I0302 13:41:51.102627       1 shared_informer.go:320] Caches are synced for ephemeral
I0302 13:41:51.118344       1 shared_informer.go:320] Caches are synced for crt configmap
I0302 13:41:51.119997       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0302 13:41:51.121889       1 shared_informer.go:320] Caches are synced for attach detach
I0302 13:41:51.121973       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0302 13:41:51.123401       1 shared_informer.go:320] Caches are synced for node
I0302 13:41:51.123432       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0302 13:41:51.123489       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0302 13:41:51.123494       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0302 13:41:51.123499       1 shared_informer.go:320] Caches are synced for cidrallocator
I0302 13:41:51.123540       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0302 13:41:51.125949       1 shared_informer.go:320] Caches are synced for expand
I0302 13:41:51.127952       1 shared_informer.go:320] Caches are synced for resource quota
I0302 13:41:51.127979       1 shared_informer.go:320] Caches are synced for PV protection
I0302 13:41:51.129978       1 shared_informer.go:320] Caches are synced for service account
I0302 13:41:51.131126       1 shared_informer.go:320] Caches are synced for namespace
I0302 13:41:51.132970       1 shared_informer.go:320] Caches are synced for deployment
I0302 13:41:51.138917       1 shared_informer.go:320] Caches are synced for HPA
I0302 13:41:51.140895       1 shared_informer.go:320] Caches are synced for garbage collector
I0302 13:41:51.142888       1 shared_informer.go:320] Caches are synced for disruption
I0302 13:41:51.144852       1 shared_informer.go:320] Caches are synced for stateful set
I0302 13:41:51.146590       1 shared_informer.go:320] Caches are synced for PVC protection
I0302 13:41:51.528359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="408.325482ms"
I0302 13:41:51.528415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="33.234¬µs"
I0302 13:42:22.232287       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.975171ms"
I0302 13:42:22.232406       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="76.591¬µs"
I0302 13:46:53.985349       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0302 13:50:24.072589       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="28.554622ms"
I0302 13:50:24.090729       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="18.105575ms"
I0302 13:50:24.090794       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="42.24¬µs"
I0302 13:50:24.097299       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="34.496¬µs"
I0302 13:50:24.104202       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="31.512¬µs"
I0302 13:50:26.728124       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="3.689576ms"
I0302 13:50:26.728174       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="22.155¬µs"
I0302 13:50:27.772971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="5.334247ms"
I0302 13:50:27.773028       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="23.275¬µs"
I0302 13:50:28.783775       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.692368ms"
I0302 13:50:28.783853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="20.072¬µs"
I0302 13:50:59.231514       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="29.391792ms"
I0302 13:50:59.240013       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="8.402824ms"
I0302 13:50:59.240109       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="69.683¬µs"
I0302 13:50:59.255291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="61.097¬µs"
I0302 13:50:59.261410       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="69.663¬µs"
I0302 13:51:01.239793       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.432357ms"
I0302 13:51:01.239853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="36.506¬µs"
I0302 13:51:02.271047       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="16.082827ms"
I0302 13:51:02.271137       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="27.596¬µs"
I0302 13:51:19.257861       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nginx-deployment-96b9d695" duration="4.401¬µs"
I0302 13:53:53.365857       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0302 13:58:59.706017       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [d3d42628da69] <==
I0224 00:00:04.238273       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0224 00:00:04.238294       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0224 00:00:04.238299       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0224 00:00:04.238304       1 shared_informer.go:320] Caches are synced for cidrallocator
I0224 00:00:04.239622       1 shared_informer.go:320] Caches are synced for TTL
I0224 00:00:04.238211       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0224 00:00:04.250278       1 shared_informer.go:320] Caches are synced for service account
I0224 00:00:04.250369       1 shared_informer.go:320] Caches are synced for stateful set
I0224 00:00:04.250382       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0224 00:00:04.250422       1 shared_informer.go:320] Caches are synced for HPA
I0224 00:00:04.257925       1 shared_informer.go:320] Caches are synced for endpoint
I0224 00:00:04.260581       1 shared_informer.go:320] Caches are synced for cronjob
I0224 00:00:04.260710       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0224 00:00:04.260730       1 shared_informer.go:320] Caches are synced for TTL after finished
I0224 00:00:04.262826       1 shared_informer.go:320] Caches are synced for crt configmap
I0224 00:00:04.262963       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0224 00:00:04.262976       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0224 00:00:04.263031       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0224 00:00:04.263040       1 shared_informer.go:320] Caches are synced for ReplicationController
I0224 00:00:04.263072       1 shared_informer.go:320] Caches are synced for resource quota
I0224 00:00:04.268921       1 shared_informer.go:320] Caches are synced for namespace
I0224 00:00:04.272197       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0224 00:00:04.272717       1 shared_informer.go:320] Caches are synced for attach detach
I0224 00:00:04.272794       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0224 00:00:04.272801       1 shared_informer.go:320] Caches are synced for GC
I0224 00:00:04.286330       1 shared_informer.go:320] Caches are synced for resource quota
I0224 00:00:04.288428       1 shared_informer.go:320] Caches are synced for job
I0224 00:00:04.290783       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0224 00:00:04.291052       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:00:04.291460       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:00:04.294196       1 shared_informer.go:320] Caches are synced for disruption
I0224 00:00:04.306612       1 shared_informer.go:320] Caches are synced for taint
I0224 00:00:04.306709       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0224 00:00:04.306749       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0224 00:00:04.306771       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0224 00:00:04.312189       1 shared_informer.go:320] Caches are synced for PVC protection
I0224 00:00:04.312521       1 shared_informer.go:320] Caches are synced for ephemeral
I0224 00:00:04.314190       1 shared_informer.go:320] Caches are synced for daemon sets
I0224 00:00:04.320744       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:00:04.346397       1 shared_informer.go:320] Caches are synced for garbage collector
I0224 00:00:04.346463       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0224 00:00:04.346471       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0224 00:00:04.347633       1 shared_informer.go:320] Caches are synced for garbage collector
I0224 00:00:04.971576       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:00:05.481329       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="714.449952ms"
I0224 00:00:05.502068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="20.569965ms"
I0224 00:00:05.503346       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="35.648¬µs"
I0224 00:00:05.503570       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="18.791¬µs"
I0224 00:00:05.541401       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="36.496¬µs"
I0224 00:00:07.975351       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="29.645¬µs"
I0224 00:00:11.918410       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:00:30.000388       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="11.28688ms"
I0224 00:00:30.000488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="27.331¬µs"
I0224 00:04:47.767474       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:09:54.139084       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:15:00.016159       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:20:06.227669       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:25:12.364797       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:30:18.187843       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0224 00:35:24.512766       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [23691dd96a71] <==
I0302 13:41:49.878755       1 server_linux.go:66] "Using iptables proxy"
I0302 13:41:50.062906       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0302 13:41:50.062993       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0302 13:41:50.085209       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0302 13:41:50.085278       1 server_linux.go:170] "Using iptables Proxier"
I0302 13:41:50.088368       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0302 13:41:50.095343       1 server.go:497] "Version info" version="v1.32.0"
I0302 13:41:50.095423       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0302 13:41:50.097999       1 config.go:199] "Starting service config controller"
I0302 13:41:50.098859       1 config.go:329] "Starting node config controller"
I0302 13:41:50.099155       1 shared_informer.go:313] Waiting for caches to sync for service config
I0302 13:41:50.099160       1 shared_informer.go:313] Waiting for caches to sync for node config
I0302 13:41:50.099340       1 config.go:105] "Starting endpoint slice config controller"
I0302 13:41:50.099345       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0302 13:41:50.199467       1 shared_informer.go:320] Caches are synced for node config
I0302 13:41:50.199485       1 shared_informer.go:320] Caches are synced for service config
I0302 13:41:50.199435       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [655fcd06df9b] <==
I0224 00:00:06.568481       1 server_linux.go:66] "Using iptables proxy"
I0224 00:00:06.780526       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0224 00:00:06.780668       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0224 00:00:06.841332       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0224 00:00:06.841386       1 server_linux.go:170] "Using iptables Proxier"
I0224 00:00:06.848965       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0224 00:00:06.862602       1 server.go:497] "Version info" version="v1.32.0"
I0224 00:00:06.862775       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0224 00:00:06.865173       1 config.go:105] "Starting endpoint slice config controller"
I0224 00:00:06.865245       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0224 00:00:06.865310       1 config.go:199] "Starting service config controller"
I0224 00:00:06.865317       1 shared_informer.go:313] Waiting for caches to sync for service config
I0224 00:00:06.865503       1 config.go:329] "Starting node config controller"
I0224 00:00:06.865508       1 shared_informer.go:313] Waiting for caches to sync for node config
I0224 00:00:06.966721       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0224 00:00:06.966816       1 shared_informer.go:320] Caches are synced for node config
I0224 00:00:06.966825       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [09296395f8ef] <==
E0223 23:59:53.950774       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.951746       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0223 23:59:53.951762       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.951902       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0223 23:59:53.951914       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0223 23:59:53.951923       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0223 23:59:53.951929       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.951975       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:53.952019       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.952028       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:53.952034       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.952173       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0223 23:59:53.952186       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.952241       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0223 23:59:53.952252       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.966062       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0223 23:59:53.968242       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.974113       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0223 23:59:53.974139       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0223 23:59:53.981128       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0223 23:59:53.981156       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0223 23:59:54.809214       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:54.809240       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:54.829550       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:54.830548       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.042999       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:55.043032       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.118352       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0223 23:59:55.124144       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.126893       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0223 23:59:55.126914       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.158898       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0223 23:59:55.164151       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.166342       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0223 23:59:55.166365       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0223 23:59:55.207055       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0223 23:59:55.207084       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.254284       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:55.254325       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.399724       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0223 23:59:55.402836       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.439890       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0223 23:59:55.439918       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.439942       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0223 23:59:55.439951       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.516510       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0223 23:59:55.528631       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.547983       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0223 23:59:55.550558       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.591728       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0223 23:59:55.617865       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0223 23:59:55.592469       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0223 23:59:55.618042       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0223 23:59:57.343628       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0223 23:59:57.353732       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0224 00:00:02.634768       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0224 00:35:40.729915       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0224 00:35:40.730039       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0224 00:35:40.730227       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0224 00:35:40.730702       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [97c02cdae151] <==
I0302 13:41:45.561364       1 serving.go:386] Generated self-signed cert in-memory
W0302 13:41:47.743249       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0302 13:41:47.743273       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0302 13:41:47.743282       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0302 13:41:47.743288       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0302 13:41:47.774103       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0302 13:41:47.774121       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0302 13:41:47.776888       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0302 13:41:47.776958       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0302 13:41:47.777552       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0302 13:41:47.779418       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0302 13:41:47.877985       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 02 13:42:11 minikube kubelet[1423]: I0302 13:42:11.531582    1423 scope.go:117] "RemoveContainer" containerID="564fda2c7f9fe08a253fab3895758f1b0eee75c65bba2cb1e8efe79233a2bf47"
Mar 02 13:42:11 minikube kubelet[1423]: I0302 13:42:11.531933    1423 scope.go:117] "RemoveContainer" containerID="993315699ba2d80125a385365cadd5e01d3af8547eb09e05de7be4969ea2db55"
Mar 02 13:42:11 minikube kubelet[1423]: E0302 13:42:11.532085    1423 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d53bce37-687b-49c0-9931-bd946ce31f89)\"" pod="kube-system/storage-provisioner" podUID="d53bce37-687b-49c0-9931-bd946ce31f89"
Mar 02 13:42:25 minikube kubelet[1423]: I0302 13:42:25.988217    1423 scope.go:117] "RemoveContainer" containerID="993315699ba2d80125a385365cadd5e01d3af8547eb09e05de7be4969ea2db55"
Mar 02 13:46:29 minikube kubelet[1423]: I0302 13:46:29.062001    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hs6gl\" (UniqueName: \"kubernetes.io/projected/d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9-kube-api-access-hs6gl\") pod \"nginx-pod\" (UID: \"d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9\") " pod="default/nginx-pod"
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.198309    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-pod" podStartSLOduration=119.530501095 podStartE2EDuration="2m6.198295392s" podCreationTimestamp="2025-03-02 13:46:28 +0000 UTC" firstStartedPulling="2025-03-02 13:46:29.420237296 +0000 UTC m=+285.759571357" lastFinishedPulling="2025-03-02 13:46:36.088031594 +0000 UTC m=+292.427365654" observedRunningTime="2025-03-02 13:46:37.121370255 +0000 UTC m=+293.460704323" watchObservedRunningTime="2025-03-02 13:48:34.198295392 +0000 UTC m=+410.537629469"
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.565403    1423 scope.go:117] "RemoveContainer" containerID="6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38"
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.576763    1423 scope.go:117] "RemoveContainer" containerID="6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38"
Mar 02 13:48:34 minikube kubelet[1423]: E0302 13:48:34.577409    1423 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38" containerID="6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38"
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.577432    1423 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38"} err="failed to get container status \"6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38\": rpc error: code = Unknown desc = Error response from daemon: No such container: 6f254d9555e6cd8c32740919e79f23d670e4976190999e21cb71786422b40d38"
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.713860    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hs6gl\" (UniqueName: \"kubernetes.io/projected/d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9-kube-api-access-hs6gl\") pod \"d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9\" (UID: \"d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9\") "
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.724994    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9-kube-api-access-hs6gl" (OuterVolumeSpecName: "kube-api-access-hs6gl") pod "d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9" (UID: "d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9"). InnerVolumeSpecName "kube-api-access-hs6gl". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:48:34 minikube kubelet[1423]: I0302 13:48:34.815123    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-hs6gl\" (UniqueName: \"kubernetes.io/projected/d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9-kube-api-access-hs6gl\") on node \"minikube\" DevicePath \"\""
Mar 02 13:48:35 minikube kubelet[1423]: I0302 13:48:35.964014    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9" path="/var/lib/kubelet/pods/d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9/volumes"
Mar 02 13:50:24 minikube kubelet[1423]: I0302 13:50:24.060255    1423 memory_manager.go:355] "RemoveStaleState removing state" podUID="d3e3f502-e9ca-45c1-beba-c8b0bf8ed7e9" containerName="nginx"
Mar 02 13:50:24 minikube kubelet[1423]: I0302 13:50:24.181858    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-x8fhx\" (UniqueName: \"kubernetes.io/projected/31b0bd37-72da-42b7-8885-147d486b836c-kube-api-access-x8fhx\") pod \"nginx-deployment-96b9d695-jzjk8\" (UID: \"31b0bd37-72da-42b7-8885-147d486b836c\") " pod="default/nginx-deployment-96b9d695-jzjk8"
Mar 02 13:50:24 minikube kubelet[1423]: I0302 13:50:24.182051    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dvfqd\" (UniqueName: \"kubernetes.io/projected/a16667b3-7378-4957-a913-3640e252e340-kube-api-access-dvfqd\") pod \"nginx-deployment-96b9d695-7qg99\" (UID: \"a16667b3-7378-4957-a913-3640e252e340\") " pod="default/nginx-deployment-96b9d695-7qg99"
Mar 02 13:50:24 minikube kubelet[1423]: I0302 13:50:24.182167    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cd8zp\" (UniqueName: \"kubernetes.io/projected/b8a82a13-a271-435a-b302-c88753a79375-kube-api-access-cd8zp\") pod \"nginx-deployment-96b9d695-4bqqb\" (UID: \"b8a82a13-a271-435a-b302-c88753a79375\") " pod="default/nginx-deployment-96b9d695-4bqqb"
Mar 02 13:50:27 minikube kubelet[1423]: I0302 13:50:27.766761    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-96b9d695-jzjk8" podStartSLOduration=1.6156605210000001 podStartE2EDuration="3.766745626s" podCreationTimestamp="2025-03-02 13:50:24 +0000 UTC" firstStartedPulling="2025-03-02 13:50:24.714799712 +0000 UTC m=+521.054133772" lastFinishedPulling="2025-03-02 13:50:26.865884818 +0000 UTC m=+523.205218877" observedRunningTime="2025-03-02 13:50:27.766632545 +0000 UTC m=+524.105966610" watchObservedRunningTime="2025-03-02 13:50:27.766745626 +0000 UTC m=+524.106079693"
Mar 02 13:50:27 minikube kubelet[1423]: I0302 13:50:27.767037    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-96b9d695-4bqqb" podStartSLOduration=2.708076591 podStartE2EDuration="3.767032524s" podCreationTimestamp="2025-03-02 13:50:24 +0000 UTC" firstStartedPulling="2025-03-02 13:50:24.698650966 +0000 UTC m=+521.037985025" lastFinishedPulling="2025-03-02 13:50:25.757606898 +0000 UTC m=+522.096940958" observedRunningTime="2025-03-02 13:50:26.724478361 +0000 UTC m=+523.063812423" watchObservedRunningTime="2025-03-02 13:50:27.767032524 +0000 UTC m=+524.106366590"
Mar 02 13:50:59 minikube kubelet[1423]: I0302 13:50:59.221022    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-96b9d695-7qg99" podStartSLOduration=31.939342185 podStartE2EDuration="35.221006625s" podCreationTimestamp="2025-03-02 13:50:24 +0000 UTC" firstStartedPulling="2025-03-02 13:50:24.728581617 +0000 UTC m=+521.067915672" lastFinishedPulling="2025-03-02 13:50:28.010246052 +0000 UTC m=+524.349580112" observedRunningTime="2025-03-02 13:50:28.779459115 +0000 UTC m=+525.118793181" watchObservedRunningTime="2025-03-02 13:50:59.221006625 +0000 UTC m=+555.560340688"
Mar 02 13:50:59 minikube kubelet[1423]: I0302 13:50:59.222845    1423 status_manager.go:890] "Failed to get status for pod" podUID="554a0204-24f8-4d27-aac3-64ed2ddd2c5d" pod="default/nginx-deployment-96b9d695-q97ng" err="pods \"nginx-deployment-96b9d695-q97ng\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"default\": no relationship found between node 'minikube' and this object"
Mar 02 13:50:59 minikube kubelet[1423]: I0302 13:50:59.287757    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-499xz\" (UniqueName: \"kubernetes.io/projected/554a0204-24f8-4d27-aac3-64ed2ddd2c5d-kube-api-access-499xz\") pod \"nginx-deployment-96b9d695-q97ng\" (UID: \"554a0204-24f8-4d27-aac3-64ed2ddd2c5d\") " pod="default/nginx-deployment-96b9d695-q97ng"
Mar 02 13:50:59 minikube kubelet[1423]: I0302 13:50:59.287916    1423 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zj5xv\" (UniqueName: \"kubernetes.io/projected/cc792fea-df7d-422c-a39d-2a43adadf0bf-kube-api-access-zj5xv\") pod \"nginx-deployment-96b9d695-gnpt7\" (UID: \"cc792fea-df7d-422c-a39d-2a43adadf0bf\") " pod="default/nginx-deployment-96b9d695-gnpt7"
Mar 02 13:51:01 minikube kubelet[1423]: I0302 13:51:01.234798    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-96b9d695-gnpt7" podStartSLOduration=1.085154525 podStartE2EDuration="2.234783869s" podCreationTimestamp="2025-03-02 13:50:59 +0000 UTC" firstStartedPulling="2025-03-02 13:50:59.760247354 +0000 UTC m=+556.099581429" lastFinishedPulling="2025-03-02 13:51:00.909876712 +0000 UTC m=+557.249210773" observedRunningTime="2025-03-02 13:51:01.23422796 +0000 UTC m=+557.573562022" watchObservedRunningTime="2025-03-02 13:51:01.234783869 +0000 UTC m=+557.574117933"
Mar 02 13:51:02 minikube kubelet[1423]: I0302 13:51:02.255560    1423 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-96b9d695-q97ng" podStartSLOduration=1.017577694 podStartE2EDuration="3.255541622s" podCreationTimestamp="2025-03-02 13:50:59 +0000 UTC" firstStartedPulling="2025-03-02 13:50:59.770250871 +0000 UTC m=+556.109584947" lastFinishedPulling="2025-03-02 13:51:02.008214817 +0000 UTC m=+558.347548875" observedRunningTime="2025-03-02 13:51:02.25493746 +0000 UTC m=+558.594271529" watchObservedRunningTime="2025-03-02 13:51:02.255541622 +0000 UTC m=+558.594875693"
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.753423    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-cd8zp\" (UniqueName: \"kubernetes.io/projected/b8a82a13-a271-435a-b302-c88753a79375-kube-api-access-cd8zp\") pod \"b8a82a13-a271-435a-b302-c88753a79375\" (UID: \"b8a82a13-a271-435a-b302-c88753a79375\") "
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.760511    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b8a82a13-a271-435a-b302-c88753a79375-kube-api-access-cd8zp" (OuterVolumeSpecName: "kube-api-access-cd8zp") pod "b8a82a13-a271-435a-b302-c88753a79375" (UID: "b8a82a13-a271-435a-b302-c88753a79375"). InnerVolumeSpecName "kube-api-access-cd8zp". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.783709    1423 scope.go:117] "RemoveContainer" containerID="f3462a1619f767dd4e99d5ef37b4a8c96950b06903d8de2862db9623375e8138"
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.854765    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-cd8zp\" (UniqueName: \"kubernetes.io/projected/b8a82a13-a271-435a-b302-c88753a79375-kube-api-access-cd8zp\") on node \"minikube\" DevicePath \"\""
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.955289    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-x8fhx\" (UniqueName: \"kubernetes.io/projected/31b0bd37-72da-42b7-8885-147d486b836c-kube-api-access-x8fhx\") pod \"31b0bd37-72da-42b7-8885-147d486b836c\" (UID: \"31b0bd37-72da-42b7-8885-147d486b836c\") "
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.957854    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/31b0bd37-72da-42b7-8885-147d486b836c-kube-api-access-x8fhx" (OuterVolumeSpecName: "kube-api-access-x8fhx") pod "31b0bd37-72da-42b7-8885-147d486b836c" (UID: "31b0bd37-72da-42b7-8885-147d486b836c"). InnerVolumeSpecName "kube-api-access-x8fhx". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:51:19 minikube kubelet[1423]: I0302 13:51:19.959871    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="b8a82a13-a271-435a-b302-c88753a79375" path="/var/lib/kubelet/pods/b8a82a13-a271-435a-b302-c88753a79375/volumes"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.056154    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zj5xv\" (UniqueName: \"kubernetes.io/projected/cc792fea-df7d-422c-a39d-2a43adadf0bf-kube-api-access-zj5xv\") pod \"cc792fea-df7d-422c-a39d-2a43adadf0bf\" (UID: \"cc792fea-df7d-422c-a39d-2a43adadf0bf\") "
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.056208    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dvfqd\" (UniqueName: \"kubernetes.io/projected/a16667b3-7378-4957-a913-3640e252e340-kube-api-access-dvfqd\") pod \"a16667b3-7378-4957-a913-3640e252e340\" (UID: \"a16667b3-7378-4957-a913-3640e252e340\") "
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.056228    1423 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-499xz\" (UniqueName: \"kubernetes.io/projected/554a0204-24f8-4d27-aac3-64ed2ddd2c5d-kube-api-access-499xz\") pod \"554a0204-24f8-4d27-aac3-64ed2ddd2c5d\" (UID: \"554a0204-24f8-4d27-aac3-64ed2ddd2c5d\") "
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.056271    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-x8fhx\" (UniqueName: \"kubernetes.io/projected/31b0bd37-72da-42b7-8885-147d486b836c-kube-api-access-x8fhx\") on node \"minikube\" DevicePath \"\""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.058379    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/a16667b3-7378-4957-a913-3640e252e340-kube-api-access-dvfqd" (OuterVolumeSpecName: "kube-api-access-dvfqd") pod "a16667b3-7378-4957-a913-3640e252e340" (UID: "a16667b3-7378-4957-a913-3640e252e340"). InnerVolumeSpecName "kube-api-access-dvfqd". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.058799    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/cc792fea-df7d-422c-a39d-2a43adadf0bf-kube-api-access-zj5xv" (OuterVolumeSpecName: "kube-api-access-zj5xv") pod "cc792fea-df7d-422c-a39d-2a43adadf0bf" (UID: "cc792fea-df7d-422c-a39d-2a43adadf0bf"). InnerVolumeSpecName "kube-api-access-zj5xv". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.059072    1423 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/554a0204-24f8-4d27-aac3-64ed2ddd2c5d-kube-api-access-499xz" (OuterVolumeSpecName: "kube-api-access-499xz") pod "554a0204-24f8-4d27-aac3-64ed2ddd2c5d" (UID: "554a0204-24f8-4d27-aac3-64ed2ddd2c5d"). InnerVolumeSpecName "kube-api-access-499xz". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.156611    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-zj5xv\" (UniqueName: \"kubernetes.io/projected/cc792fea-df7d-422c-a39d-2a43adadf0bf-kube-api-access-zj5xv\") on node \"minikube\" DevicePath \"\""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.156655    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-dvfqd\" (UniqueName: \"kubernetes.io/projected/a16667b3-7378-4957-a913-3640e252e340-kube-api-access-dvfqd\") on node \"minikube\" DevicePath \"\""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.156666    1423 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-499xz\" (UniqueName: \"kubernetes.io/projected/554a0204-24f8-4d27-aac3-64ed2ddd2c5d-kube-api-access-499xz\") on node \"minikube\" DevicePath \"\""
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.803524    1423 scope.go:117] "RemoveContainer" containerID="f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.833851    1423 scope.go:117] "RemoveContainer" containerID="f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869"
Mar 02 13:51:20 minikube kubelet[1423]: E0302 13:51:20.835479    1423 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869" containerID="f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.835523    1423 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869"} err="failed to get container status \"f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869\": rpc error: code = Unknown desc = Error response from daemon: No such container: f6910490244922a33b09fedfa739f3b2b1e9178e0f161ba3ce795c0033100869"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.835548    1423 scope.go:117] "RemoveContainer" containerID="a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.859843    1423 scope.go:117] "RemoveContainer" containerID="a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740"
Mar 02 13:51:20 minikube kubelet[1423]: E0302 13:51:20.862425    1423 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740" containerID="a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.862471    1423 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740"} err="failed to get container status \"a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740\": rpc error: code = Unknown desc = Error response from daemon: No such container: a804b468479fb88ec4cb66cc16ffca435706204dbf209793b5d55a5d94c20740"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.862550    1423 scope.go:117] "RemoveContainer" containerID="59648c5c1846ce3fdbb1c8b9db8ba165ae29d0d48470d2c592b666cf1aae3809"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.883443    1423 scope.go:117] "RemoveContainer" containerID="ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.903417    1423 scope.go:117] "RemoveContainer" containerID="ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c"
Mar 02 13:51:20 minikube kubelet[1423]: E0302 13:51:20.904082    1423 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c" containerID="ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c"
Mar 02 13:51:20 minikube kubelet[1423]: I0302 13:51:20.904193    1423 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c"} err="failed to get container status \"ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c\": rpc error: code = Unknown desc = Error response from daemon: No such container: ae36d031d99e9fb8a6fe94c905f227aa3e57548c01eabd5c3495bd521fe39c1c"
Mar 02 13:51:21 minikube kubelet[1423]: I0302 13:51:21.958626    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="31b0bd37-72da-42b7-8885-147d486b836c" path="/var/lib/kubelet/pods/31b0bd37-72da-42b7-8885-147d486b836c/volumes"
Mar 02 13:51:21 minikube kubelet[1423]: I0302 13:51:21.959228    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="554a0204-24f8-4d27-aac3-64ed2ddd2c5d" path="/var/lib/kubelet/pods/554a0204-24f8-4d27-aac3-64ed2ddd2c5d/volumes"
Mar 02 13:51:21 minikube kubelet[1423]: I0302 13:51:21.959711    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="a16667b3-7378-4957-a913-3640e252e340" path="/var/lib/kubelet/pods/a16667b3-7378-4957-a913-3640e252e340/volumes"
Mar 02 13:51:21 minikube kubelet[1423]: I0302 13:51:21.960054    1423 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="cc792fea-df7d-422c-a39d-2a43adadf0bf" path="/var/lib/kubelet/pods/cc792fea-df7d-422c-a39d-2a43adadf0bf/volumes"


==> storage-provisioner [993315699ba2] <==
I0302 13:41:49.726527       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0302 13:42:10.756182       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [ea3754f3dd73] <==
I0302 13:42:26.133806       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0302 13:42:26.143503       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0302 13:42:26.143900       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0302 13:42:43.559948       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0302 13:42:43.560293       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8f666276-78df-452e-98c6-b3b0c0ac5f6b", APIVersion:"v1", ResourceVersion:"2240", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_4da94d8c-c583-484d-9dbd-979508b58c78 became leader
I0302 13:42:43.560586       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_4da94d8c-c583-484d-9dbd-979508b58c78!
I0302 13:42:43.661886       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_4da94d8c-c583-484d-9dbd-979508b58c78!

